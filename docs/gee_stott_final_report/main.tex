\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{url}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Rylee: Training a Neural Network to Play Chess Like a Human}

\author{\IEEEauthorblockN{Ethan Gee and Nate Stott}
\IEEEauthorblockA{\textit{Computer Science, Utah State University}\\
ethan.gee@usu.edu, nate.stott@usu.edu}
}

\maketitle

\begin{abstract}
Artificial Intelligence (AI) has surpassed human performance in many domains; since 2005, chess engines have demonstrated consistent superhuman play. While human-aligned AI has proven beneficial, fields such as law, medicine, and human resources cannot directly deploy superhuman AI due to ethical constraints. These limitations, however, do not preclude the use of AI systems designed to complement---rather than exceed---human performance. Prior work shows that aligning with human-observable behavior, instead of explaining behavior, can yield desirable results. Chess provides a strong testbed for developing such systems.
Rylee is a neural-network-based chess engine trained to mimic human move selection, aiming for lightweight computation and modest predictive accuracy. Traditional chess engines, optimized for perfect play, offer limited value as human training tools, and attempts to attenuate them fail to capture human-like behavior. Maia improves on this but requires substantial compute to train and run. This work builds on Maia by developing a more computationally efficient, human-aligned model---comparable in size to Stockfish and capable of running on standard hardware. Trained on large Lichess datasets across multiple rating levels, Rylee learns to predict human moves with fidelity. Key results show that TODO and experiments demonstrate that TODO. The broader significance of this research lies in advancing human-aligned AI and enabling more accessible, engaging AI-based training systems across diverse domains.
\end{abstract}

\begin{IEEEkeywords}
Chess, Neural Networks, Machine Learning, Artificial Intelligence, AI, Human-like AI, Deep Learning
\end{IEEEkeywords}

\section{Introduction}
Since the 1940s, chess-playing programs, or ``chessbots,'' have evolved to surpass the capabilities of the most skilled human players. These bots are now integral to modern chess training, with players globally using them to refine their skills. However, a significant disparity exists between the strategies employed by traditional chessbots and human players. Human chess is characterized by the use of heuristics, intuition, and pattern recognition. In contrast, conventional chessbots rely on brute-force computation, analyzing vast numbers of potential future board states to select the move that maximizes their probability of winning.

This computational superiority creates an imbalance in human-computer matches, where the bot's expansive memory and processing power provide a decisive advantage. To mitigate this, bots are often programmed to introduce deliberate errors, resulting in an unnatural and often confusing experience for the human player. This presents a paradox: while chessbots dominate the game, their non-human-like mode of play limits their effectiveness as a learning tool for humans.

This project addresses the challenge of creating a more ``human-like'' chessbot. Our work builds upon the research of McIlroy-Young et al. on the MAIA models, which were trained using supervised learning to emulate human decision-making, a departure from the reinforcement learning approach common to engines like AlphaZero. While MAIA demonstrated strong performance in predicting human moves, its reliance on the large and resource-intensive AlphaZero architecture limits its practical application. Conversely, Stockfish, a significantly smaller engine, achieves comparable performance to AlphaZero, indicating that model size does not scale linearly with performance.

The primary objective of this project is to develop a chessbot that not only plays like a human but is also computationally efficient enough to run on standard consumer hardware. We aim to achieve a level of human-like play comparable to the MAIA models but with a model deployable to edge devices. To this end, we will utilize the Lichess Dataset, a comprehensive collection of human chess games and player ratings, which is the same dataset employed in the training of MAIA. This work contributes to the growing field of human-aligned AI by exploring the trade-offs between model size, performance, and human-like behavior in the domain of chess.

\section{Related Work}

\subsection{Chess Engines}

While early chess engines relied on traditional search algorithms like minimax and alpha-beta pruning, the field has been transformed by neural network approaches. These early engines, including Deep Blue, used handcrafted evaluation functions that required extensive manual tuning of positional and material factors.

The introduction of deep neural networks marked a paradigm shift in computer chess. Modern engines employ sophisticated architectures: AlphaZero uses a deep residual network with 20 residual blocks, each containing convolutional layers and batch normalization. This network processes the 8x8x73 board representation to output both a position evaluation and a policy distribution over possible moves. Leela Chess Zero (LC0), an open-source implementation of similar principles, utilizes a network with 24 residual blocks and generates 256 channels in its intermediate layers.

Maia, specifically designed for human-like play, employs a modified version of LC0's architecture. Instead of learning through self-play, Maia is trained on millions of human games using supervised learning. Its network consists of multiple convolutional layers followed by a policy head that predicts move probabilities and a value head that evaluates positions. Maia processes the board state through 12 input planes (6 piece types $\times$ 2 colors) and incorporates additional features like player ratings. Recent hybrid approaches, such as Stockfish's NNUE (Efficiently Updatable Neural Network), combine traditional search with a smaller neural network containing roughly 40,000 parameters that can be efficiently updated during search.

\subsection{Human-Like Chess AI}

While the pursuit of optimal play has been the primary focus of chess engine development, there is a growing interest in creating AI that plays in a more human-like manner. The Maia project, by McIlroy-Young et al., represents a significant step in this direction. Instead of using reinforcement learning to find the best possible move, Maia is trained on a large dataset of human games to predict the move a human player would make in a given position. This supervised learning approach results in an engine that exhibits more human-like characteristics, including making mistakes that are typical of human players at a certain skill levels instead of the always the optimal move. Importantly, the model is never required to intentionally choose a sub-optimal move. Because the task is reframed as predicting the move a human would make, the notion of an ``optimal'' move in the traditional engine sense no longer applies. This leads to experiences humans can easily learn from.

\subsection{Dataset and Training Data}

The Lichess database is a massive, publicly available collection of chess games played on the Lichess.org platform. It contains billions of games played by humans of all skill levels, making it an invaluable resource for training human-like chess models. The games are stored in the Portable Game Notation (PGN) format, which includes the moves of the game, the players' ratings (Elo), the game's outcome and other statistics.

\section{Proposed Method}

\subsection{System Architecture}

We structured our work on the chess engine Rylee around three core components: data processing, model training, and deployment into a playable environment. First, we automated the download of Lichess data files and passed them through a preparation pipeline, converting them into standardized, training-ready formats and storing the processed results in a database. During training, the model draws directly from this preprocessed dataset, eliminating the need to re-run the preparation pipeline and allowing us to iterate on model architecture and training configurations efficiently. Finally, the trained model is deployed into an interactive environment where it can be evaluated through live play.

\subsection{Data Collection and Processing}

\subsubsection{PGN File Processing}

Lichess data is stored in the form of PGN files. PGN or Portable Game Notation is made up of 2 parts the tag pairs, which stores game metadata about the game and players, and the move strings of the game stored in UCI.

To make this data usable we stored the players elos, which is a measure of their skill. Then we iterate through the game storing ``snapshots'' of the board at every state, and the move taken by the current player at that position. The board states are then represented as 3 dimensional tensors that are 8 by 8 (due to the size of the board) with 12 channels with 6 channels for each of the white pieces and 6 for the black.

\subsection{Neural Network Architecture}

The neural network architecture consists of three main components: a convolutional backbone for processing board states, a fully connected network for feature extraction, and dual output heads for move prediction and auxiliary tasks.

The convolutional backbone processes the 8x8x12 board representation through six convolutional layers. Each layer uses 64 filters with 8x8 kernels, maintaining the spatial dimensions through ``same'' padding. ReLU activation functions are applied after each convolution to introduce non-linearity.

The fully connected portion begins with a 4100-dimensional input (flattened board features plus metadata) and progressively reduces dimensionality through six layers (512$\rightarrow$32$\rightarrow$32$\rightarrow$32$\rightarrow$32$\rightarrow$32) with ReLU activations. This creates a compact 32-dimensional representation of the game state.

The network splits into two parallel output heads:
\begin{enumerate}
\item Move prediction head: Maps the 32-dimensional state to 2104 move probabilities
\item Auxiliary prediction head: Produces additional chess-relevant predictions
\end{enumerate}

Input representation:
\begin{itemize}
\item Board state: 8x8x12 tensor (6 piece types $\times$ 2 colors)
\item Metadata: Player ratings and game state information
\item Combined input: 4100 dimensions (4096 from board + 4 metadata features)
\end{itemize}

Output representation:
\begin{itemize}
\item 2104-dimensional probability distribution over legal moves
\item Softmax activation ensures valid probability distribution
\item Auxiliary head provides additional game state predictions to augment learning process
\end{itemize}

The architecture balances computational efficiency with sufficient complexity to capture chess patterns and human play styles. The dual head design allows simultaneous learning of move prediction and auxiliary chess concepts.

\subsection{Training Process}

\subsubsection{Implementation Details}

The training implementation utilizes PyTorch's ecosystem for deep learning. The training pipeline is encapsulated in a Trainer class that handles data loading, model training, evaluation, and checkpointing. Key components include:

\begin{itemize}
\item Data loading with PyTorch DataLoaders for efficient batch processing
\item Adam optimizer with configurable learning rate and weight decay
\item Cross-entropy loss for both move prediction and valid moves
\item Automatic checkpointing and performance logging
\item GPU acceleration when available
\end{itemize}

\subsubsection{Training Parameters}

The model was trained with the following configuration:
\begin{itemize}
\item Batch size: 512
\item Initial learning rate: 0.001
\item Weight decay: 1e-4
\item Beta parameters: (0.9, 0.999)
\item Number of epochs: 50
\item Data split: 80\% training, 10\% validation, 10\% test
\end{itemize}

\subsubsection{Optimization Strategy}

We employed random search for hyperparameter optimization, exploring combinations of:
\begin{itemize}
\item Learning rates: [0.1, 0.01, 0.001, 0.0001]
\item Weight decay rates: [1e-3, 1e-4, 1e-5]
\item Beta values: [0.9, 0.95, 0.99]
\item Momentum values: [0.9, 0.95, 0.99]
\end{itemize}

The best performing configuration was selected based on validation loss.

\subsubsection{Hardware Configuration}

Training was conducted on embedded AI hardware:

\begin{itemize}
\item Platform: NVIDIA Jetson Orin Nano 8GB Developer Kit
\item GPU: 1024-core NVIDIA Ampere architecture GPU
\item CPU: 6-core Arm Cortex-A78AE v8.2 64-bit CPU
\item RAM: 8GB 128-bit LPDDR5
\item Storage: 64GB eMMC 5.1
\end{itemize}

This setup processed approximately 10,000 positions per second during training.

\section{Experiments}

\subsection{Dataset Description}

We trained our model off of 823,000 games from January to May of 2013. These games were played by a wide breadth of players in a variety of states. The original maia model was split into 9 separate models split apart by elo ranges of 100 (e.g. 1100-1200) this limited the models generalizability and so we trained our model on one complete dataset to increase access. Exanding from 1100-1900 to 700-2500 to further increase access.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/elo-distribution.png}}
\caption{Spread of the elos present in the training set.}
\label{fig:elo}
\end{figure}

One other key difference is the fact that both iterations of the maia paper are unable to play chess openings as they don't have any knowledge of the first 10 moves. This is a profound limitation as the opening is one of the key areas where players struggle within a game. So knowing this we choose to include the first 10 moves to increase the understanding of openings.

We also didn't limit the model to just the blitz games as we wanted to increase the overall variability in the games that the model was capable of representing. We introduced this noise as it gives a far more holistic view of human players. This was useful as blitz games only show up a small portion of the data and we want to show all human play.

\begin{figure}[htbp]
\centerline{\includegraphics[width=\linewidth]{figures/time_control_distribution.png}}
\caption{Time Control Splits of the games from Jan-May of 2013}
\label{fig:timecontrol}
\end{figure}

The Lichess dataset used in this project consists of over 1 billion chess games played on the Lichess.org platform between 2013-2023. Key characteristics:

\begin{itemize}
\item Total size: ~2TB of compressed game data
\item Game format: Portable Game Notation (PGN)
\item Rating range: 800-2900 ELO
\item Game types: Classical, Rapid, Blitz, and Bullet time controls
\item Key fields per game:
\begin{itemize}
\item Player ELO ratings
\item Move sequences in UCI format
\item Game result
\item Time control
\item Opening classification
\item Timestamps
\end{itemize}
\end{itemize}

For training, we took data from January to May of 2013 consisting of 860,000 games from players rated 797-2412 ELO to focus on typical club-level play patterns. Games were validated to remove incomplete or corrupted entries.

Data processing pipeline:
\begin{enumerate}
\item PGN parsing and validation
\item Feature extraction (board states, moves, metadata)
\item Train/validation/test splitting (80/10/10)
\item Normalization of numerical features
\end{enumerate}

\subsection{Baseline Description}

All of the source code for this project is a available at \url{https://github.com/EthanDGee/ryleeeeeeeeeeeee}

\subsection{Experimental Evaluation}

TODO

\section{Conclusion and Future Work}

\subsection{Summary}

% TODO: Brief recap of the project
% TODO: Restate key achievements
% TODO: Final assessment of success

\subsection{Future Work}

\subsubsection{Model Improvements}

% TODO: Larger/deeper networks
% TODO: Alternative architectures (transformers, etc.)
% TODO: Transfer learning approaches

\subsubsection{Training Enhancements}

% TODO: Larger datasets
% TODO: Data augmentation techniques
% TODO: Multi-task learning

\subsubsection{Feature Additions}

% TODO: Opening book integration
% TODO: Endgame tablebase support
% TODO: Rating-specific models

\subsubsection{Evaluation Extensions}

% TODO: User studies with human players
% TODO: Turing test for chess
% TODO: More sophisticated human-likeness metrics

\subsection{Concluding Remarks}

% TODO: Final thoughts on the project
% TODO: Broader context and significance

\begin{thebibliography}{00}
\bibitem{b1} R. McIlroy-Young, S. Sen, J. Kleinberg, and A. Anderson, ``Aligning Superhuman AI with Human Behavior: Chess as a Model System,'' in Proc. 26th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD), 2020.
\bibitem{b2} D. Silver et al., ``Mastering the game of Go with deep neural networks and tree search,'' Nature, vol. 529, pp. 484--489, 2016.
\bibitem{b3} D. Silver et al., ``A general reinforcement learning algorithm that masters chess, shogi, and Go through self-play,'' Science, vol. 362, no. 6419, pp. 1140--1144, 2018.
\bibitem{b4} Z. Tang, D. Jiao, R. McIlroy-Young, J. Kleinberg, S. Sen, and A. Anderson, ``Maia-2: A Unified Model for Human-AI Alignment in Chess,'' in Proc. 38th Conf. on Neural Information Processing Systems (NeurIPS), 2024.
\bibitem{b5} R. McIlroy-Young, S. Sen, J. Kleinberg, and A. Anderson, ``Aligning Superhuman AI with Human Behavior: Chess as a Model System,'' in Proc. 26th ACM SIGKDD Int. Conf. on Knowledge Discovery and Data Mining (KDD), 2020, pp. 1667--1677.
\bibitem{b6} T. McGrath et al., ``Acquiring human-like concepts from a superhuman AI,'' arXiv:2210.13432, 2022.
\bibitem{b7} R. McIlroy-Young, R. Wang, A. Anderson, and J. Kleinberg, ``Detecting Individual Decision-Making Style: Exploring Behavioral Stylometry in Chess,'' in Proc. 35th Conf. on Neural Information Processing Systems (NeurIPS), 2021.
\bibitem{b8} A. Paszke et al., ``PyTorch: An Imperative Style, High-Performance Deep Learning Library,'' in Advances in Neural Information Processing Systems 32, H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch√©-Buc, E. Fox, and R. Garnett, Eds. Curran Associates, Inc., 2019, pp. 8024--8035.
\bibitem{b9} N. Moskopp, ``python-chess: a chess library for Python,'' GitHub repository, 2014. [Online]. Available: \url{https://github.com/niklasf/python-chess}
\bibitem{b10} Doshi-Velez, F., \& Kim, B. (2017). A Roadmap for a Rigorous Science of Interpretability. ArXiv, abs/1702.08608.
\bibitem{b11} Stadie, B. C., Abbeel, P., \& Sutskever, I. (2017). Third-person imitation learning. arXiv preprint arXiv:1703.01703.
\bibitem{b12} Charness, N. The impact of chess research on cognitive science. Psychol. Res 54, 4--9 (1992). \url{https://doi.org/10.1007/BF01359217}
\end{thebibliography}

\end{document}
